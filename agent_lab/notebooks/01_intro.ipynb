{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 1- Introduction**\n",
    "\n",
    "Welcome to the first step of this journey to your (maybe) first AI Agent.  \n",
    "  \n",
    "We are are going to build a *\"Food ordering agent\"*.  \n",
    "\n",
    "To give you an example: imagine a site where, once you have registered, you can, using natural language, ask for daily menu, order food, check the status of your orders, cancel an order and so on. \n",
    " \n",
    "*Example*\n",
    "\n",
    "![chat demo](../docs/images/chat-demo.png)\n",
    "\n",
    "\n",
    "Each notebook can be run independently and it builds on the previous one, so you will see the same code repeated several times, once you get familiar with it just run the cells and skip to the part relevant to the step you're in.\n",
    "\n",
    "If you're interested, you can find [more information about the use case](../docs/understanding-the-use-case.md).\n",
    "\n",
    "\n",
    "## **Let's start**\n",
    "\n",
    "The cell below contains the initial code you need to interact with the LLM.  Familiarize yourself with the code and run it using the SHIFT + ENTER keys. \n",
    "\n",
    "If you are using VS Code, you can run the cells using the little arrow to the left of the cell:  \n",
    "\n",
    "<img src=\"../docs/images/run_cell.png\" width=\"300\"> \n",
    "\n",
    "For those not familiar, the following code initializes a client which handles communication with Azure OpenAI and defines a generic function `invoke_llm` that can be used to send requests to the LLM.\n",
    "\n",
    "(If an error is observed running the cell below, it is likely due to incorrect environment variable naming.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.5' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/local/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# 1: In this cell we just setup the environment and import the necessary libraries and modules to interact with the LLM.\n",
    "# If you already played with OpenAI this code will look familiar to you, if not you can have a quick look.\n",
    "\n",
    "import os\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "from termcolor import cprint\n",
    "\n",
    "# Environment setup\n",
    "load_dotenv()\n",
    "api_key=os.environ['AZURE_OPENAI_API_KEY']\n",
    "api_version=os.environ['AZURE_OPENAI_API_VERSION']\n",
    "deployment=os.environ['AZURE_OPENAI_DEPLOYMENT']\n",
    "\n",
    "#Initialize AzureOpenAI client\n",
    "client = AzureOpenAI(\n",
    "  api_key=api_key,  \n",
    "  api_version = api_version\n",
    "  )\n",
    "\n",
    "# The function we use to invoke the LLM\n",
    "def invoke_llm(system_message:str, user_message:str)->str:\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}, {\"role\": \"user\", \"content\": user_message}]\n",
    "    response = client.chat.completions.create(\n",
    "      model=deployment,    \n",
    "      messages = messages,    \n",
    "      temperature=0.3) # The temperature range is from 0 to 2. Lower values for temperature results in more consistent outputs, while higher values generate more diverse and creative results (e.g. 1.0).\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send a message to the LLM \n",
    "\n",
    "In the next cell we have prepared examples of a `system_message` and `user_message` for you to run.  \n",
    "Feel free to edit them, if you want, and then run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mI'm sorry, but I'm currently unavailable to assist with food delivery orders today as it's Monday. Please feel free to reach out any other day between 9 AM and 9 PM for assistance.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Here is the predefined system prompt (change it as you wish! ðŸ™‚)\n",
    "system_message = f'''\n",
    "You are an assistant with expertise in providing food delivery information. You are open everyday from 9 AM to 9 PM except Mondays. It's Monday today.\n",
    "'''\n",
    "\n",
    "# Here is the user request (just an example, feel free to change it! ðŸ™‚)\n",
    "user_message=\"I would like to order a pizza today at 11 PM\"\n",
    "\n",
    "# Let's invoke the LLM\n",
    "response = invoke_llm(system_message, user_message)\n",
    "cprint(response, 'yellow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything is fine you should see the response from the LLM.  \n",
    "\n",
    "If not, check that the values you added to the `.env` file are the right ones as per the [prerequisites](../README.md).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Time for an easy challenge!** ðŸ˜Ž\n",
    "\n",
    "Let's assume that the service you are going to build is open everyday **from 9 AM to 9 PM except Mondays**.\n",
    "\n",
    "Can you change the above `system_message` and `user_message` in a way that the LLM is able to answer questions related to the working schedule?\n",
    "\n",
    "*Examples*  \n",
    "- *Are you open now?*\n",
    "- *What is your working schedule?*\n",
    "- *May I have a pizza delivered tomorrow at 1 PM?*\n",
    "- *I'd like to order a pizza for Saturday at 7:30 PM*\n",
    "\n",
    "\n",
    "Take some time experimenting, editing and re-running the cell above to test your solution.\n",
    "\n",
    "Once done, go to [Notebook 2](02_get_schedule.ipynb) where we'll have a look at possible solution."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "2.7.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
