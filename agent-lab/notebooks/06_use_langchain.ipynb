{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 6 - Let's go LangChain**\n",
    "\n",
    "**Disclaimer ‚ùó**  \n",
    "\n",
    "This step is **not** meant to be a LangChain tutorial, nor is it expected that you become proficient with it.\n",
    "\n",
    "The goal is to show an example of an Agent implemented using one of the popular frameworks to appreciate some of the advantages a framework can provide.  \n",
    "\n",
    "The decision to pick LangChain was driven by its popularity and the fact that its agent impelementation is close to what you've already seen so far, so it should look quite familiar.  \n",
    "\n",
    "**Let's start then!**  \n",
    "\n",
    "The first cell, apart the use of LangChain's [AzureChatOpenAI](https://python.langchain.com/v0.1/docs/integrations/chat/azure_chat_openai/) client is the same as before, you can simply run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pytz\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Imports\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.tools import tool\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "from datetime import datetime\n",
    "from services.data_provider import DataProvider\n",
    "from termcolor import cprint\n",
    "from langchain.agents import AgentExecutor, create_openai_functions_agent\n",
    "from typing import Optional\n",
    "\n",
    "# Environment setup\n",
    "load_dotenv()\n",
    "\n",
    "api_key=os.environ['AZURE_OPENAI_API_KEY']\n",
    "endpoint=os.environ['AZURE_OPENAI_ENDPOINT']\n",
    "deployment=os.environ['AZURE_OPENAI_DEPLOYMENT']\n",
    "api_version=os.environ['AZURE_OPENAI_API_VERSION']\n",
    "local_timezone = os.environ['TIMEZONE']\n",
    "\n",
    "llm=AzureChatOpenAI(api_key=api_key, \n",
    "                    azure_endpoint=endpoint,\n",
    "                    azure_deployment=deployment,\n",
    "                    api_version=api_version, \n",
    "                    temperature=0.3)\n",
    "\n",
    "# This the data access layer\n",
    "db:DataProvider=DataProvider()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is exactly the same `system_message` of the previous step, again just run the cell..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The system message didn't change...\n",
    "system_message = f'''\n",
    "Role: An assistant with expertise in handling questions about operative schedule and available menu of a food delivery service.\n",
    " \n",
    "Instructions:\n",
    "- Provide information about the available menu and the opening schedule.\n",
    "- Kindly deny any request not regarding opening schedule and available menu.\n",
    "- Always assume the date and time returned by the get_current_datetime tool as the reference date for all date calculations.\n",
    "- If a date is not indicated use the reference date.\n",
    "- If a time is not indicated assume it is 12 PM.\n",
    "- Assume the following format: 'DD/MM/YYYY HH:MM' as the standard format for dates.\n",
    "\n",
    "Steps:\n",
    "  - Opening Schedule:\n",
    "      1. Always use the calculated date based on reference date to check and return the opening schedule.\n",
    "      2. Always indicate the desired data and time including weekday in the response.\n",
    "      \n",
    "  - Available menu:\n",
    "      1. When the user asks for the a menu you should respond using the following template enclosed in triple quotes:\n",
    "\n",
    "      ```\n",
    "      Menu for <day-name> <requested date>\n",
    "      \n",
    "      Pasta\n",
    "      \n",
    "      1. <name> - <price> - <ingredients> - <label>\n",
    "      2. <name> - <price> - <ingredients> - <label>\n",
    "      3. ...\n",
    "      4. ..      \n",
    "      \n",
    "      \n",
    "      Pizza\n",
    "      \n",
    "      1. <name> - <price> - <ingredients> - <label>\n",
    "      2. <name> - <price> - <ingredients> - <label>\n",
    "      3. ...\n",
    "      4. .. \n",
    "      \n",
    "      Today's Special Pizzas\n",
    "      \n",
    "      1. <name> - <price> - <ingredients> - <label>\n",
    "      2. <name> - <price> - <ingredients> - <label>\n",
    "      3. ...\n",
    "      4. ..   \n",
    "      \n",
    "      Drinks\n",
    "      \n",
    "      1. <name> - <price> \n",
    "      2. <name> - <price> \n",
    "      3. ...\n",
    "      4. .. \n",
    "      \n",
    "      Dessert\n",
    "      \n",
    "      1. <name> - <price> - <ingredients> - <label>\n",
    "      2. <name> - <price> - <ingredients> - <label>\n",
    "      3. ...\n",
    "      4. .. \n",
    "      ```  \n",
    "      \n",
    "      2. If the request is about specific menu entries (e.g. 'do you have pasta', 'do you offer vegetarian food' or 'are there any specials?' ), together with the menu kindle answer the question.     \n",
    "      3. If no specials are available for the day, do not include that section into menu.\n",
    "      4. If no menu is available, kindly reply that there you can serve any food for that day..\n",
    "      5. If the request relates to an item that is not available in the menu, kindly reply that it cannot be ordered.      \n",
    "  \n",
    "Expectation:\n",
    "  - Provide a seamless experience to the user, by providing the requested information in a kind and timely manner, including all the necessary details and guiding the user to a possible follow up step.\n",
    "\n",
    "Narrowing:  \n",
    "  1. Deny all the requests referring to a date antecedent the reference date.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the function arguments using LangChain\n",
    "\n",
    "LangChain uses [Pydantic](https://docs.pydantic.dev/latest/) types to define the schema of the input parameters.  \n",
    "  \n",
    "So we are going to define two classes `ScheduleToolInputSchema` and  `DailyMenuInputSchema` which will represent the parameters our functions expect.  \n",
    "  \n",
    "We use a [Field](https://docs.pydantic.dev/latest/concepts/fields/) to provide the description of each parameter and  [Optional](https://docs.python.org/3/library/typing.html) to indicate fields which are not mandatory.\n",
    "\n",
    "If you compare them with the information in the Tools definition of the previous step, you will see many similarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lanchain uses Pydantic to define the input schema for the tools, no\n",
    "\n",
    "# Define the input schema for the schedule-tool\n",
    "class ScheduleToolInputSchema(BaseModel):\n",
    "    date: str = Field(description=\"The date for which you want to know the schedule in format 'DD/MM/YYYY HH:MM' (e.g. '31/12/2023 15:00').\")\n",
    "    user_id: Optional[str] = Field(description=\"The optional user id in form '#<4-digits-number>'.\")\n",
    "    \n",
    "# Define the input schema for daily menu tool\n",
    "class DailyMenuInputSchema(BaseModel):\n",
    "    date: str = Field(description=\"The date for which you want to know the available menu in format 'DD/MM/YYYY HH:MM' (e.g. '29/05/2024 17:00').\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define LangChain Tools using the @tool decorator \n",
    "\n",
    "Following are the exact same code function you've seen in the previous steps.  \n",
    "\n",
    "The only difference here is that we identify them as [Tools](https://python.langchain.com/v0.1/docs/modules/tools/) by using the [@tool](https://api.python.langchain.com/en/latest/tools/langchain_core.tools.tool.html#langchain_core.tools.tool) decorator and we provide the description of the function as doc-string.  \n",
    "\n",
    "The function is linked to the input definitions we have seen before via its `arg_schema` property.  \n",
    "\n",
    "LangChain offer [several alternatives](https://python.langchain.com/v0.2/docs/how_to/custom_tools/) to define tools, this is the simplest one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the operative schedule for the provided date and the optional user id\n",
    "@tool(\"schedule-tool\", args_schema=ScheduleToolInputSchema)\n",
    "def operative_schedule(date:str, user_id:str=None):\n",
    "    \"\"\"Useful when you need to know the operative schedule on a specific date and time.\"\"\"\n",
    "    weekday= datetime.strptime(date, \"%d/%m/%Y %H:%M\").strftime(\"%A\")    \n",
    "    \n",
    "    # Check if the user is a special client\n",
    "    is_special = db.is_special_client(user_id)\n",
    "    # Get the operative schedule from the database\n",
    "    schedule=db.get_schedule(day=weekday, is_special=is_special) \n",
    "    # Check if the requested date/timw comes within the working schedule       \n",
    "    if schedule.status == \"open\":        \n",
    "        time= datetime.strptime(date, \"%d/%m/%Y %H:%M\").time()\n",
    "        opening = datetime.strptime(schedule.start, \"%H:%M\").time()\n",
    "        closing = datetime.strptime(schedule.end, \"%H:%M\").time()        \n",
    "        if time < opening or time > closing:\n",
    "            schedule.status = \"closed\"            \n",
    "        \n",
    "    return schedule\n",
    "\n",
    "# Returns the menu for the provided date\n",
    "@tool(\"daily-menu-tool\", args_schema=DailyMenuInputSchema)\n",
    "def get_menu(date:str):\n",
    "    \"\"\"Useful when you need to know the menu for a specific date.\"\"\"\n",
    "    target_date= datetime.strptime(date, \"%d/%m/%Y %H:%M\") \n",
    "    weekday= target_date.strftime(\"%A\")\n",
    "    \n",
    "    # Get the menu from the database\n",
    "    return db.get_menu(day=weekday)\n",
    "\n",
    "# Function to get the current date and time in the format dd/mm/yyyy hh:mm\n",
    "@tool(\"current-datetime-tool\")\n",
    "def get_current_datetime():\n",
    "    \"\"\"Useful when you need to know the current date and time for date and time based operations.\"\"\"        \n",
    "    local_tz = pytz.timezone(local_timezone)\n",
    "    return {\n",
    "        \"current_datetime\": datetime.now(local_tz).strftime(\"%A, %d/%m/%Y %H:%M\")\n",
    "    } "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we need to use them later, we create an array of tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create an array containing the tools that will be used by the agent\n",
    "tools=[operative_schedule, get_menu, get_current_datetime]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an Agent to using the LangChain AgentExecutor\n",
    "\n",
    "In the next cell we create a function `run_agent` in charge of processing the passed `user_request`.\n",
    "\n",
    "Let go through it:  \n",
    "1. A Langchain [prompt](https://python.langchain.com/v0.1/docs/modules/model_io/prompts/) object is created from a series of messages, the `weird` one is probaly the [MessagePlaceHoler](https://python.langchain.com/v0.1/docs/modules/model_io/prompts/quick_start/#messagesplaceholder) that, as the name says, it's a placeholder used by LangChain to add the 'invoke function' and relative response messages you have seen before.  \n",
    "\n",
    "2. The [create_openai_function_agent](https://python.langchain.com/v0.1/docs/templates/openai-functions-agent/) creates an agent grouping the LLM, the Tools and the prompt (remember the diagram from [step 05](05_get_menu_v2.ipynb)?  \n",
    "\n",
    "3. [AgentExecutor](https://python.langchain.com/v0.1/docs/modules/agents/concepts/#agentexecutor) is the runtime in charge of calling the tools, adding the returned data into message history, call the llm again, etc.  \n",
    "\n",
    "4. Invokes the agent passing the user request. (note that prompt definition uses a `{input}` placeholder)\n",
    "\n",
    "\n",
    "If all this look quite complicated, **no worries** (and welcome to LangChain world üôÇ), just remember that this does exactly what you did before under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Creates the agent, the framework to run it and runs it using provided user request\n",
    "def run_agent(user_request:str):\n",
    "    # Create a prompt object made of system message, user input and a placeholder for the agent to store information returned by the tools\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",system_message),        \n",
    "        (\"user\", \"{input}\"), # {input} is a placeholder for the user input passed via invoke method of AgentExecutor\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"), \n",
    "    ])\n",
    "\n",
    "    # Create the agent\n",
    "    agent = create_openai_functions_agent(llm, tools, prompt)\n",
    "    # Create the runtime environment for the agent\n",
    "    agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True) # verbose=True will print the messages exchanged between the agent and the tools\n",
    "    # Execute the agent\n",
    "    response= agent_executor.invoke({\"input\": user_request})\n",
    "    return response[\"output\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoke the Agent and observe the results \n",
    "\n",
    "Here you can finally see the agent in action!  \n",
    "\n",
    "Play with `user_message` and see the output, thanks to the `verbose=True` setting of [AgentExecutor](https://python.langchain.com/v0.1/docs/modules/agents/concepts/#agentexecutor) you can see what the agent is doing in the background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Here is the input message from the user, as usual play with it to see how the agent responds\n",
    "user_message = \"what can i order next Tuesday?\" \n",
    "\n",
    "\n",
    "cprint(f\"Input: {user_message}\", \"blue\") \n",
    "# Run the agent with the user message\n",
    "response = run_agent(user_message)\n",
    "cprint (response, \"yellow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations, you now have a LangChain agent! üòé\n",
    "\n",
    "**Note:**  \n",
    "  \n",
    "Doesn't matter if you did not get all the LangChain stuff, what matters is to remember that in real-world project, most of the time, all the orchestration between the LLM and its tools is done by a framework, each one in a different way, but with the same goal.\n",
    "\n",
    "Time to continue to next step: [Notebook 07](07_order_tool.ipynb) where we'll add the tool in charge of handling orders."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
