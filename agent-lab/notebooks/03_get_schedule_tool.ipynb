{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 3 - Let's use a Tool**\n",
    "\n",
    "Let's now brainstorm a possible solution to [Step 2's challenge](02_get_schedule.ipynb).  \n",
    "\n",
    "Due to the potentially large number of VIP customers involved (potentially thousands), the idea of including all the special users IDs in the prompt is clearly not feasible.  \n",
    "\n",
    "<!-- An alternative could be to ask GPT to analyze the presence of an ID in the prompt and, if found, return it, then identify somehow if that ID belongs to a VIP customer and return (with or without the help of the LLM) the proper schedule. \n",
    "\n",
    "This could potentially work, but imagine the user asking: *What is the schedule today for 730* (meaning 7:30 PM) would the LLM consider 730 as the ID?, or will correctly return \"no id found\" and what if in our solution we will have to handle with several different cases like this, like when the user enters it's id, the number of coke he wishes, the time in form *\"in 20 minutes*\"   \n",
    "\n",
    "And how would we discriminate a request for the daily schedule for user 8643 from a request to cancel an order 8643? -->\n",
    "\n",
    "### **Change the mindset**\n",
    "\n",
    "Until now, we have tried to find a solution by using the LLM as tool to help us get what we need from the user (user's ID in this case).  \n",
    "\n",
    "What if, instead, we instruct the LLM how to complete a specific task (e.g. Handle requests for the operating schedule) and give it the opportunity to \"ask\" for additional details in case it thinks that the amount of available information is not enough to complete it?  \n",
    "\n",
    "Sounds complicated? üò≤\n",
    "\n",
    "As an example, imagine giving the following *pseudo*-instructions to the LLM.    \n",
    "\n",
    "- You are an assistant in charge of handling requests about the opening schedule of a restaurant.\n",
    "- Your task is to reply to requests coming from a user.\n",
    "- In case you need to know if the restaurant is open or closed on a specific date, respond with \"need schedule <the-date-you-need-to-check>\"\n",
    "- In this case I will fetch the schedule for that date and I will give it back to you.\n",
    "- Then continue elaborating the request.\n",
    "- Feel free to ask for schedules as many times you think is necessary.\n",
    "\n",
    "Do you see the difference?   \n",
    "\n",
    "We are now asking the LLM to **reason** on a given task and, if it deems necessary, **tell us** to **provide** it more information based on certain criteria; so that once that information is provided to it, the LLM can continue reasoning and complete the assigned task.\n",
    "\n",
    "Ok, but *how can this be done technically?*\n",
    "\n",
    "Let's see it in action.  We start with our usual AzureOpenAI client & data provider. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import json\n",
    "import datetime\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "from services.data_provider import DataProvider\n",
    "from termcolor import cprint\n",
    "\n",
    "# Environment setup\n",
    "load_dotenv()\n",
    "api_key=os.environ['AZURE_OPENAI_API_KEY']\n",
    "deployment=os.environ['AZURE_OPENAI_DEPLOYMENT']\n",
    "api_version=os.environ['AZURE_OPENAI_API_VERSION']\n",
    "\n",
    "#Initialize AzureOpenAI client\n",
    "client = AzureOpenAI(\n",
    "  api_key=api_key,  \n",
    "  api_version = api_version\n",
    "  )\n",
    "\n",
    "# This the data access layer\n",
    "db:DataProvider=DataProvider()\n",
    "\n",
    "messages = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure the system message \n",
    "\n",
    "Have now a look at the `system message`, as you see is now more structured and divided into sections that clearly describe the task that must be solved. \n",
    "\n",
    "However, we still have not introduced the concept that the LLM can ask us for additional information to help it accomplish the task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System message (We use RISEN syntax https://easyaibeginner.com/risen-framework-ai-prompt-for-chatgpt/)\n",
    "system_message = f'''\n",
    "Role: An assistant with expertise in handling questions about operative schedule of a food delivery service.\n",
    " \n",
    "Instructions:\n",
    "- Provide information about the opening schedule.\n",
    "- Kindly deny any request not regarding opening schedule.\n",
    "- Always assume the date and time provided as input the reference date for all date calculations.\n",
    "- If a date is not indicated use the reference date.\n",
    "- If a time is not indicated assume it is 12 PM.\n",
    "- Assume the following format: 'DD/MM/YYYY HH:MM' as the standard format for dates.\n",
    "\n",
    "Steps:\n",
    "  - Opening Schedule:\n",
    "      1. Always use the calculated date based on reference date to check and return the opening schedule.\n",
    "      2. Always indicate the desired data and time including weekday in the response.\n",
    "  \n",
    "Expectation:\n",
    "  - Provide a seamless experience to the user, by providing the requested information in a kind and timely manner, including all the necessary details and guiding the user to a possible follow up step.\n",
    "\n",
    "Narrowing:  \n",
    "  1. Deny all the requests referring to a date antecedent the reference date.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing a function to look up the schedule for a particular user on a given day \n",
    "\n",
    "You will find below a function that will return the schedule for a given day in JSON format based a particular `date` and an **optional** `user id`.  For example, it will return the following schedule for Tuesday 25th June 2024:\n",
    "\n",
    "```json\n",
    "{\n",
    "      \"day\": \"Tuesday\",\n",
    "      \"start\": \"9:00\",\n",
    "      \"end\": \"21:00\",\n",
    "      \"status\": \"open\"\n",
    "}\n",
    "```\n",
    "You don't need to understand the intricacies of the function.  What is important to remember is: what **goes in** and the corresponding **output**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def operative_schedule(date:str, user_id:str=None):\n",
    "    \"\"\"\n",
    "    Retrieves and operating schedule for a given date and user.\n",
    "\n",
    "    This function determines the operative schedule for a specified date. It first extracts the weekday from the provided date. \n",
    "    Then, it checks if the user is a special client and retrieves the schedule from the database accordingly. \n",
    "    If the schedule status is \"open,\" the function verifies whether the requested date/time falls within the schedule's operational hours.\n",
    "    If the time is outside the operational hours, it updates the schedule status to \"closed.\"\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    date : str\n",
    "        The date and time in the format \"dd/mm/YYYY HH:MM\" for which to check the schedule.\n",
    "    user_id : str, optional\n",
    "        The unique identifier of the user. This is used to determine if the user is a special client. Default is None.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    str\n",
    "        A JSON string representing the schedule model with updated status, if applicable.\n",
    "\n",
    "    Notes:\n",
    "    -----\n",
    "    - The function relies on a database (`db`) with methods `is_special_client(user_id)` and `get_schedule(day, is_special)`.\n",
    "    - The `get_schedule` method returns a schedule object with properties `status`, `start`, `end`, and `model_dump_json()`.\n",
    "    - The function assumes the provided `date` string is in the correct format and valid.\n",
    "    \n",
    "    Example:\n",
    "    -------\n",
    "    >>> operative_schedule(\"23/06/2024 14:30\", \"1234\")\n",
    "    '{\"status\": \"open\", \"start\": \"09:00\", \"end\": \"17:00\"}'\n",
    "    \"\"\"\n",
    "    weekday= datetime.strptime(date, \"%d/%m/%Y %H:%M\").strftime(\"%A\")    \n",
    "    \n",
    "    # Check if the user is a special client\n",
    "    is_special = db.is_special_client(user_id)\n",
    "    # Get the operative schedule from the database\n",
    "    schedule=db.get_schedule(day=weekday, is_special=is_special) \n",
    "    # Check if the requested date/time is within the working schedule, if not we update the status to closed.      \n",
    "    if schedule.status == \"open\":        \n",
    "        time= datetime.strptime(date, \"%d/%m/%Y %H:%M\").time()\n",
    "        opening = datetime.strptime(schedule.start, \"%H:%M\").time()\n",
    "        closing = datetime.strptime(schedule.end, \"%H:%M\").time()        \n",
    "        if time < opening or time > closing:\n",
    "            schedule.status = \"closed\"            \n",
    "        \n",
    "    return schedule.model_dump_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the function\n",
    "\n",
    "It's just a standard Python function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "operative_schedule('27/6/2024 00:00') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How can we inform the LLM that this function is available? \n",
    "\n",
    "As you might have noticed, in the system_message defined before there is no mention of the part: \n",
    "- *In case you think you need to know if the restaurant is open or closed on a specific date, respond with \"need schedule <the-date-you-need-to-check>\"*  \n",
    "\n",
    "\n",
    "**How do we communicate this to the LLM?** \n",
    " \n",
    "For this we have to use a technique called [function calling](https://platform.openai.com/docs/guides/function-calling) that ChatGPT describes as:  \n",
    "\n",
    "> The capability of the model to interact with external functions or APIs during a conversation or while processing a query.   \n",
    "This enables the LLM to extend its capabilities beyond just generating text based on its training data, allowing it to perform specific tasks or retrieve real-time information through predefined functions\n",
    "\n",
    "In short it means that we can tell the LLM to ask us (our code) to invoke a specific function with specified parameter values and then provide the result back to it.\n",
    "\n",
    "\n",
    "## Ok, but *how do I do that?*\n",
    "\n",
    "We do it by defining an object with a well defined structure like the one you see in the next cell.  (The format is an extended JSON schema)\n",
    "\n",
    "Have a look and try to see what kind of information is present. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the available functions to the LLM to use\n",
    "tools = [\n",
    "        {        \n",
    "            \"type\": \"function\",    \n",
    "            \"function\": {\n",
    "                \"name\": \"operative_schedule\",\n",
    "                \"description\": \"Useful when you need to know the operative schedule on a specific date.\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"date\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The date for which you want to know the schedule in format 'DD/MM/YYYY HH:MM' (e.g. '31/12/2023 15:00').\",\n",
    "                        },\n",
    "                        \"user_id\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The optional user id in form '#<4-digits-number>'.\",\n",
    "                        }                        \n",
    "                    },\n",
    "                    \"required\": [\"date\"]\n",
    "                }\n",
    "            },            \n",
    "        }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What you see is that the JSON represents:\n",
    "- An array of functions.\n",
    "- Each function has:\n",
    "  - A **Name** (*\"operative_schedule\"*)\n",
    "  - A **Description** (*\"Useful when you need to know the operative schedule on a specific date.\"*)\n",
    "  - A **Variable number of parameters**, each one with its own **data type** and **descripion**\n",
    "  - A list indicating which parameters are **mandatory** (*\"date\"*)\n",
    "\n",
    "Note:\n",
    "- The name of the function in the JSON matches the name of the function that we defined before. It's not a requirement, but makes the code easier to understand üòè\n",
    "- The list of parameters also matches the ones we have in the preceding code.  \n",
    "\n",
    "<br>\n",
    "\n",
    "## Now, include the tool descriptions when invoking the LLM \n",
    "\n",
    "On it we have the same function you have already seen in the previous notebooks but with the following changes:\n",
    "\n",
    "1. The input parameter is optional and when missing the LLM just uses what's on the messages list.\n",
    "2. **Important**‚ùóWe now assign the `tools` object created before to the `tools` property of the `create` method.\n",
    "\n",
    "So now our chat object is aware of:  \n",
    "\n",
    "1. The **system message**\n",
    "2. The **user message**\n",
    "3. What **functions** it might request we call if necessary (via the `tools` object)\n",
    "\n",
    "üí° While **function calling** is a general terms that applies to any language model, OpenAI uses the term **Tools**, so each time you read this term just think about a **code function** that has to be invoked.  \n",
    "\n",
    "Now run the cell and continue..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invokes the LLM with the provided input\n",
    "def invoke_llm(input:str=None)->str:\n",
    "    if input:\n",
    "          messages.append({'role': 'user', 'content': input})\n",
    "    response = client.chat.completions.create(\n",
    "    model=deployment,    \n",
    "    messages = messages,\n",
    "    tools=tools,\n",
    "    tool_choice=\"auto\", # see https://platform.openai.com/docs/guides/function-calling/function-calling-behavior for more details,    \n",
    "    temperature=0.7)\n",
    "\n",
    "    return response.choices[0].message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap \n",
    "\n",
    "Here's a representation of what we have build so far.  \n",
    "\n",
    "<img src=\"../docs/images/agent-and-code.png\" width=\"800\"> \n",
    "\n",
    "As you see, there is something still unclear: *Who* is going to call the `operative_schedule` function, get the result and feed the result back into the LLM?\n",
    "\n",
    "If you are thinking the LLM, the response is NO. The LLM is and remains stateless.  Instead the LLM informs you of the function and arguments to use, and you call the function. \n",
    "\n",
    "<br>\n",
    "\n",
    "## Understanding the sequence \n",
    "\n",
    "What is going to happen is visible in the following sequence diagram  \n",
    "\n",
    "<img src=\"../docs/images/tool-flow.png\" width=\"800\">\n",
    "\n",
    "1. When the user submits a question, it gets forwarded to the LLM.\n",
    "2. Based on the **system_message**, **tools** and **messages** content, the LLM will decide that it needs to 'invoke' the `operative_schedule` function.\n",
    "3. It will then reply with a special message containing the *call_id*, the *function name* and the optional *parameters* to use.\n",
    "4. The returned message is added to the messages history, so that the LLM knows that it has placed this request.\n",
    "4. The code will then invoke the `operative_schedule` function with the provided arguments and get the returned value.\n",
    "5. A new message containing: role = `tool`, id=`call_id`, the `**function name** and the **response value** is created and added to the messages history.\n",
    "6. The LLM is invoked again, without any user message, forcing the LLM to reason using **system_message**, **tools** and the updated list of **messages**\n",
    "7. Since no more information is needed to complete the task, the LLM will provide the response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below does exactly what described.  \n",
    "  \n",
    "Since this is what happens under the hood of the whole Agent story, spend some time playing with it (you can re-run the cell as many times you want), check the content of the LLM response, edit the prompt (e.g. adding **'I am user 1234'**), etc. \n",
    "\n",
    "If you are using VS Code, try debugging the cell code using the **Debug Cell** option so you test the whole flow. \n",
    "  \n",
    "<img src=\"../docs/images/debug-cell.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the user request, play with it! \n",
    "user_message = \"are you open tomorrow at 7:30 PM? I am user 1234\"\n",
    "\n",
    "messages = [{'role': 'system', 'content': system_message}]\n",
    "\n",
    "now = datetime.now().strftime(\"%A, %d/%m/%Y %H:%M\")\n",
    "input_request = f\"Given that now is {now}, handle the following user request enclosed in triple backticks: ```{user_message}```\" \n",
    "cprint(f\"Input: {input_request}\", \"blue\")  \n",
    "\n",
    "# Pass the request to the LLM\n",
    "response = invoke_llm(input=input_request)\n",
    "\n",
    "# Check if the response contains the request to invoke the functions\n",
    "if response.tool_calls:\n",
    "    cprint(response, \"cyan\")\n",
    "    # Note that we add the request from the LLM to the messages list\n",
    "    messages.append(response)\n",
    "    # We iterate over the tool calls to invoke the functions (it is possible to have multiple functions in the same response)\n",
    "    for tool_call in response.tool_calls:        \n",
    "        if tool_call.function.name == \"operative_schedule\":\n",
    "            tool_args = json.loads(tool_call.function.arguments)  \n",
    "            cprint (f\"Invoking function: {tool_call.function.name}...\", \"green\")\n",
    "            # We invoke the function and get the response\n",
    "            function_response=operative_schedule(**tool_args)\n",
    "            cprint (f\"Function call completed...\", \"yellow\")\n",
    "            # We now add the response from the function to the messages list, note the use of the tool_call.id to let the model link the response to the function call\n",
    "            messages.append(\n",
    "                {\n",
    "                    \"tool_call_id\": tool_call.id,\n",
    "                    \"role\": \"tool\",\n",
    "                    \"name\": tool_call.function.name,\n",
    "                    \"content\": function_response,\n",
    "                })      \n",
    "                  \n",
    "    cprint (f\"Control back to the LLM...\", \"magenta\")    \n",
    "    response = invoke_llm()\n",
    "    cprint (response.content, \"yellow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WOW** ü•µ    \n",
    "\n",
    "This has been a hard one!  \n",
    "\n",
    "But now you are familiar with LLM Function Calling and know what makes Agents work, and, contrary to popular belief, know that the LLM is **not** invoking the functions for you. Your code does.\n",
    "\n",
    "‚ùó**Important**  \n",
    "\n",
    "The LLM uses the information provided in the `tools` object to understand which tool to invoke, so using a detailed description for both function and parameters is **crucial** to help the LLM understand when (or not) to invoke a particular tool.\n",
    "\n",
    "Since the `tools` object become part of the prompt, keep in mind that it contributes to the whole request token count, so don't make descriptions extremely long.\n",
    "\n",
    "In the [Notebook 4](04_get_menu_tool.ipynb) we're going to add another tool for getting the daily menu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
